{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "208b80d6-0f53-46f9-b0c6-75e95a9b6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42e5c3b0-c8b6-4e10-a20e-53a614a19a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "news_dataset = pd.read_csv(r'C:\\Users\\monil\\Desktop\\Graduate Project\\resources\\datasets\\train.csv\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc073793-c367-47af-87c3-81767d860b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values with empty string\n",
    "news_dataset = news_dataset.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ac0fa0d-3cf0-43f0-8662-fadf61d384f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Downsample Dataset to 8,000 While Keeping Proportions\n",
    "# Get separate DataFrames for each label\n",
    "fake_news = news_dataset[news_dataset[\"label\"] == 1]\n",
    "real_news = news_dataset[news_dataset[\"label\"] == 0]\n",
    "\n",
    "# Get equal proportions (around 4,000 each)\n",
    "fake_sample = fake_news.sample(n=4000, random_state=42)\n",
    "real_sample = real_news.sample(n=4000, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_dataset = pd.concat([fake_sample, real_sample]).sample(frac=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21cb5af0-553d-4529-bf54-60baa7cdf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = balanced_dataset[\"text\"].values\n",
    "Y = balanced_dataset[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bebbb853-9298-4703-84c4-8b7f681ff6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfa3bd51-94a1-456a-b716-35df9696c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer from local directory\n",
    "tocken_name = r\"C:\\Users\\monil\\Desktop\\Graduate Project\\resources\\BERT Model\\bert-base-uncased\"  # Path to the downloaded files\n",
    "tokenizer = BertTokenizer.from_pretrained(tocken_name)\n",
    "\n",
    "model_checkpoint = r\"C:\\Users\\monil\\Desktop\\Graduate Project\\Fake-News-Detection\\results\\checkpoint-1200\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33a7dd66-bd5a-4852-8bba-954f84f0b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_data(texts, labels):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4f2ff-59ca-4b4e-b20b-4d2867bb3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenize_data(X_train.tolist(), Y_train.tolist())\n",
    "test_encodings = tokenize_data(X_test.tolist(), Y_test.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87b33b2-cac0-4215-a890-b27bda1ad062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Move Encodings to GPU\n",
    "train_encodings = {key: val.to(\"cuda\") for key, val in train_encodings.items()}\n",
    "test_encodings = {key: val.to(\"cuda\") for key, val in test_encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a19ca97-00c3-48ee-b213-e4419731ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create PyTorch Dataset\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = {key: val.cpu() for key, val in encodings.items()}  # Ensure CPU tensors\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Ensure long tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, Y_train.tolist())\n",
    "test_dataset = NewsDataset(test_encodings, Y_test.tolist())\n",
    "\n",
    "\n",
    "# # Save the tokenized dataset\n",
    "torch.save(train_dataset, \"./train_dataset.pt\")\n",
    "torch.save(test_dataset, \"./test_dataset.pt\")\n",
    "\n",
    "## After restarting the kernal load the datasets which were previously saved\n",
    "# train_dataset = torch.load(\"./train_dataset.pt\",weights_only=False)\n",
    "# test_dataset = torch.load(\"./test_dataset.pt\",weights_only=False)\n",
    "\n",
    "# Now you can use train_dataset and test_dataset directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88314d82-3c66-4b42-9872-aa71095673fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the GPU\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2bb2d07-981a-4bfd-be95-21ce8bfc7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Ensure save and eval strategy match\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,  # Added this to prevent memory issues\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable Mixed Precision Training for Faster Training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c77d9ffb-8c14-4eee-adff-b47bd1b60444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80413a1f-a3ad-4159-bea0-9c0fe8303851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 5:30:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.070194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.066765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.070457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=0.06055707385142644, metrics={'train_runtime': 19845.2492, 'train_samples_per_second': 0.967, 'train_steps_per_second': 0.06, 'total_flos': 5051732262912000.0, 'train_loss': 0.06055707385142644, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6279cabf-9f97-4e9d-aebb-a33a073bebd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\monil\\\\Desktop\\\\Graduate Project\\\\resources\\\\BERT Model\\\\bert-base-uncased\\\\fine_tuned_bert_tokenizer\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\monil\\\\Desktop\\\\Graduate Project\\\\resources\\\\BERT Model\\\\bert-base-uncased\\\\fine_tuned_bert_tokenizer\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\monil\\\\Desktop\\\\Graduate Project\\\\resources\\\\BERT Model\\\\bert-base-uncased\\\\fine_tuned_bert_tokenizer\\\\vocab.txt',\n",
       " 'C:\\\\Users\\\\monil\\\\Desktop\\\\Graduate Project\\\\resources\\\\BERT Model\\\\bert-base-uncased\\\\fine_tuned_bert_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(r\"C:\\Users\\monil\\Desktop\\Graduate Project\\resources\\BERT Model\\bert-base-uncased\\fine_tuned_bert_model\")\n",
    "tokenizer.save_pretrained(r\"C:\\Users\\monil\\Desktop\\Graduate Project\\resources\\BERT Model\\bert-base-uncased\\fine_tuned_bert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac59a9d1-cb1f-44d6-86b8-d45bfc11e26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 07:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eval_loss', 'eval_model_preparation_time', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second'])\n",
      "Test Accuracy: Key not found\n",
      "Evaluation Results: {'eval_loss': 0.07045724242925644, 'eval_model_preparation_time': 0.0077, 'eval_runtime': 440.9766, 'eval_samples_per_second': 3.628, 'eval_steps_per_second': 0.227}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results.keys())\n",
    "print(\"Test Accuracy:\", results.get(\"accuracy\", \"Key not found\"))\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08b5e8c3-3f0f-491f-a555-5ac9e7feb58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9869\n"
     ]
    }
   ],
   "source": [
    "# Move model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Prepare data loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Store predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)  # Get predicted class\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")  # Print accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0220d1d7-2828-429f-a8e5-6537f98b1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fake_news(text):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move to GPU if available\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits.cpu().numpy()  # Get raw logits\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()  # Convert to probabilities\n",
    "\n",
    "    print(f\"Logits: {logits}\")\n",
    "    print(f\"Probabilities: {probs}\")\n",
    "\n",
    "    # Get predicted class\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    labels = [\"Fake News\", \"Real News\"]  # Adjust if your dataset is labeled differently\n",
    "    return labels[predicted_class], probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b548ed2f-ca78-4085-a7d9-104292f97b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: [[-3.984375  4.34375 ]]\n",
      "Probabilities: [[2.415663e-04 9.997584e-01]]\n",
      "Text: Scientists confirm the moon is actually made of cheese! -> Prediction: ('Real News', array([[2.415663e-04, 9.997584e-01]], dtype=float32))\n",
      "Logits: [[-3.6210938  4.0546875]]\n",
      "Probabilities: [[4.6371284e-04 9.9953628e-01]]\n",
      "Text: New study shows drinking water makes people immortal. -> Prediction: ('Real News', array([[4.6371284e-04, 9.9953628e-01]], dtype=float32))\n",
      "Logits: [[-3.9003906  4.3632812]]\n",
      "Probabilities: [[2.5764457e-04 9.9974233e-01]]\n",
      "Text: Breaking: A man found a time machine in his basement! -> Prediction: ('Real News', array([[2.5764457e-04, 9.9974233e-01]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"Scientists confirm the moon is actually made of cheese!\",\n",
    "    \"New study shows drinking water makes people immortal.\",\n",
    "    \"Breaking: A man found a time machine in his basement!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"Text: {text} -> Prediction: {predict_fake_news(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af0bc162-f5d5-476b-9518-cf2e02bef9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News: 3200, Real News: 3200\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(Y_train.tolist())  # Convert labels to numpy array\n",
    "fake_count = (labels == 0).sum()\n",
    "real_count = (labels == 1).sum()\n",
    "\n",
    "print(f\"Fake News: {fake_count}, Real News: {real_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab8d23a3-615e-4538-b424-0a5215b49604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: [[-4.03125  4.375  ]]\n",
      "Probabilities: [[2.2341634e-04 9.9977654e-01]]\n",
      "Text: qwertyuiop asdfghjkl zxcvbnm 1234567890 -> Prediction: Real News, Probabilities: [[2.2341634e-04 9.9977654e-01]]\n",
      "Logits: [[-3.2285156  3.4472656]]\n",
      "Probabilities: [[0.0012595 0.9987405]]\n",
      "Text: This is completely made-up and has no basis in reality. -> Prediction: Real News, Probabilities: [[0.0012595 0.9987405]]\n",
      "Logits: [[-3.9042969  4.1640625]]\n",
      "Probabilities: [[3.1319875e-04 9.9968684e-01]]\n",
      "Text: Aliens have landed in New York and are taking selfies with humans! -> Prediction: Real News, Probabilities: [[3.1319875e-04 9.9968684e-01]]\n",
      "Logits: [[-3.6503906  3.9804688]]\n",
      "Probabilities: [[4.8500832e-04 9.9951494e-01]]\n",
      "Text: A magical unicorn was found in the Amazon rainforest! -> Prediction: Real News, Probabilities: [[4.8500832e-04 9.9951494e-01]]\n",
      "Logits: [[-3.65625   4.015625]]\n",
      "Probabilities: [[4.6552691e-04 9.9953449e-01]]\n",
      "Text: Government officials admit to time travel experiments. -> Prediction: Real News, Probabilities: [[4.6552691e-04 9.9953449e-01]]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"qwertyuiop asdfghjkl zxcvbnm 1234567890\",\n",
    "    \"This is completely made-up and has no basis in reality.\",\n",
    "    \"Aliens have landed in New York and are taking selfies with humans!\",\n",
    "    \"A magical unicorn was found in the Amazon rainforest!\",\n",
    "    \"Government officials admit to time travel experiments.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    prediction, probs = predict_fake_news(text)\n",
    "    print(f\"Text: {text} -> Prediction: {prediction}, Probabilities: {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba5fb6-4e09-4e89-ad23-a98740b4f44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
